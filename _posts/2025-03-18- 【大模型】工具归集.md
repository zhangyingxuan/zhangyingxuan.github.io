## 知识库私有化搭建 - macos

### 使用 Cherry Studio + ollama 结合 deepseek、bge-m3 打造个人知识库

> 对于任何本地知识库问答的过程，都需要经历文本切割、词向量化、词向量存储、词向量检索与问答几个基本环节。

#### 第一步：模型准备

1. 下载 ollama 地址https://ollama.com/
2. 本地化部署 deepseek-r1

```shell
# 下载LLM模型
ollama pull deepseek-r1:1.5b
# 下载嵌入模型
ollama pull bge-m3
# 运行LLM模型（非必须）本地运行使用
ollama run deepseek-r1:1.5b
```

3. 开启远程访问

```shell
# 1. 打开配置文件
vi /etc/systemd/system/ollama.service
# 2. 在Service中增加下面两行
Environment="OLLAMA_HOST=0.0.0.0"
Environment="OLLAMA_ORIGINS=*"
# 3. 重启ollama服务
systemctl daemon-reload
systemctl restart ollama
```

#### 第二步：模型配置

1. 下载[CherryStudio](https://cherry-ai.com/)
   > CherryStudio 是一个支持多平台的 AI 客户端，支持 Win、macOS、Linux 平台,未来也会支持移动端。项目自 24 年 7 月至今已迭代数百个版本,我们致力于打造一个更加高效、安全、易用的客户端，让更多人能够享受到 AI 带来的便利。

#### 第三步：web 端 ai 助手

1. Page Assist 通过浏览器插件来访问本地部署的大模型，这个插件还支持本地/远程知识库搭建。仅支持 ollama 体系， 安装插件 Page Assist，搜索插件后添加至 Chrome。
2. chatboxai，chatboxai 仅支持本地知识库，无法链接远程服务器，且功能单一只能作为日常问答使用，https://web.chatboxai.app/

#### 第四步：工作流编排

1. 工作流工具平台选择，n8n、Dify、coze，选用开源工具 n8n
2. 安装 n8n 并运行

```shell
# 创建名为n8n_data的持久存储（卷）
docker volume create n8n_data
# 生成名为n8n的Docker容器和镜像
docker run -it --rm --name n8n -p 5678:5678 -v n8n_data:/home/node/.n8n docker.n8n.io/n8nio/n8n
```

#### 安装 MCP n8n 社区节点

## 基础知识

### 常用工具

- Ollama
  Ollama 是一个本地化的机器学习框架，专注于自然语言处理（NLP）任务，如模型加载、推理和生成。它允许用户在本地部署和运行大型预训练模型，从而实现高效的文本生成、翻译、问答等功能。
- 硅基流动（Siliconflow）
  硅基流动（Siliconflow）是一个大模型 API 聚合平台，提供了多种大语言模型（LLM）的 API 接口服务。它支持多种模型，如 DeepSeek R1 和 DeepSeek V3 等。用户可以通过注册获取 API 密钥，并在不同的客户端（如 Cherry Studio）中使用这些模型。
- Cherry Studio
  Cherry Studio 是一款集多模型对话、知识库管理、AI 绘画、翻译等功能于一体的全能 AI 助手平台。它支持多种大型语言模型（LLM）提供商的服务，包括 OpenAI、Gemini、Anthropic、DeepSeek 等，并且可以与 Ollama 集成，实现本地模型的部署和使用

- Anything LLM
  Anything LLM 是一款集成度非常高、且功能非常稳定的大模型前端产品，虽无法进行二次开发，但是功能实现便捷，适合小白使用。下载地址
  https://anythingllm.com

#### 智能体工具

1. coze
   coze（中文名称：扣子）是一个 AI 应用开发平台，字节跳动旗下的 AI 产品。coze 提供了友好的可视化设计和编排工具，无论你是否具备编程能力，都可以通过低代码方式拖拖拽拽，然后基于自己业务或者需求快速搭建出基于大模型的各类 AI 项目；

同时支持将 AI 应用发布到各个社交平台、通讯软件，也可以通过 API 或 SDK 将 AI 应用集成到你的业务系统中。 2. Dify

### 模型分类

- 对话模型
  对话模型就是能和人聊天的模型。由 ollama 调度 DeepSeek 完成对话；比如你问一个聊天机器人“今天天气怎么样？”它会回答“今天天气很好，适合出门。”它的任务就是根据你的问题，生成一个合适的回答。

- Embedding 模型：负责进行词向量化的模型
- 词向量数据库：用于存储词向量化后的对象，默认为 LancelotDB

- 嵌入模型
  嵌入模型是把文字变成数字向量的模型。这是生成知识库的内核

- 文生图
  Cherry Studio 自带多种文生图模型，比如 Stable Diffusion，Janus Pro 等，省去了你在本地搭建服务的繁琐成本，太省力了有木有！

- RAG（Retrieval-Augmented Generation，检索增强生成）
  是一种结合检索和生成的先进自然语言处理（NLP）技术，用于提升生成模型的能力。它通过引入外部知识库或文档检索机制，增强了生成模型的准确性和信息丰富性，特别适用于需要结合外部知识的任务，比如问答、对话系统和知识密集型文本生成。

#### 为什么 RAG

| 问题       | 传统 LLM                   | RAG 知识库             |
| ---------- | -------------------------- | ---------------------- |
| 知识更新   | 训练后固定，无法访问新数据 | 实时检索，获取最新信息 |
| 私有知识   | 只能访问公开数据           | 可接入企业/个人数据    |
| 回答可信度 | 可能出现幻觉，无法追溯来源 | 结合检索，提供真实依据 |
| 搜索能力   | 依赖关键字，无法理解语义   | 语义搜索，提高精确度   |
| 数据格式   | 仅限文本输入               | 支持 PDF、数据库、API  |

🚀 RAG 是未来 AI 知识库的核心技术，它让 AI 更聪明、更精准、更可信！如果你希望 AI 高效管理知识、精准回答问题、提升业务效率，RAG 绝对是值得采用的方案。

#### 模型类型对比

| 类型               | 特点                                                                                       | DeepSeek          | OpenAI       | Zhipu            | Alicloud    |
| ------------------ | ------------------------------------------------------------------------------------------ | ----------------- | ------------ | ---------------- | ----------- |
| 大语言模型 LLM     | 只能文本输入和输出，通用任务能力较强，同时具有很好的工具调用能力。这是所有其他模型的基础。 | DeepSeek V3       | GPT-4⁺       | glm-4-plus       | Qwen-Max⁺   |
| 多模态模型 VLLM⁺   | 除了文字外，能够理解图片或者视频，能够输出文本。                                           | DeepSeek VL       | GPT-4o       | glm-4v-plus      | Qwen-VL-Max |
| 推理模型 Reasoning | 擅长处理复杂问题，具有较强的推理能力                                                       | DeepSeek R1       | OpenAI-o1    | glm-zero-preview | n/a         |
| 代码模型 CodeGen⁺  | 擅长生成代码                                                                               | DeepSeek Coder V2 | OpenAI Codex | CodeGeeX         | 通义灵码    |

### 知识库对比

| 特性/平台  | MaxKb                         | AnythingLLM                        | Dify                                       | FastGPT                             | RagFlow                         | 其他                  |
| ---------- | ----------------------------- | ---------------------------------- | ------------------------------------------ | ----------------------------------- | ------------------------------- | --------------------- |
| 主要用途   | 知识库管理与检索              | 大规模语言模型的应用开发           | AI 应用快速开发与部署                      | 快速构建和部署 GPT 模型             | 基于检索增强生成（RAG）的工作流 | LlamaIndex、OpenWebUI |
| 支持的模型 | 自定义                        | 支持多种开源及私有 LLMs            | 主要支持轻量级本地运行的模型               | GPT 系列模型                        | 支持通过 API 集成的各种模型     | -                     |
| 硬件要求   | 适中，根据模型大小变化        | 高，特别是对于大型模型             | 较低，优化用于本地部署                     | 根据模型大小变化，可能较高          | 中等到高，取决于使用的模型      | -                     |
| 用户友好度 | 提供图形界面（GUI），易于使用 | 对开发者友好，但需要一定的技术背景 | 提供 CLI 和 API 接口，相对易用             | 提供简化工具，适合快速原型设计      | 需要一定的配置和技术知识        | -                     |
| 集成能力   | 可以与其他系统集成            | 强大的集成能力，支持多环境         | 良好的集成能力                             | 专注于 GPT 模型的快速集成           | 强调与外部数据源和模型的集成    | -                     |
| 扩展性     | 支持扩展，但有限              | 高度可扩展                         | 具有一定的扩展性                           | 适用于快速扩展的小型到中型项目      | 设计为高度可扩展                | -                     |
| 适用场景   | 知识管理、文档检索            | 复杂的语言处理任务，如对话系统     | 快速开发 AI 应用，适合初创公司和个人开发者 | 快速原型设计和小项目的 GPT 模型部署 | 需要深度文本理解和检索的场景    | -                     |
| 社区和支持 | 社区较小，官方支持有限        | 活跃的开源社区                     | 新兴平台，社区正在增长                     | 相关资源丰富，尤其是关于 GPT 的内容 | 开源项目，有一定活跃度          | -                     |
